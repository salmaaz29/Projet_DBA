# tests/test_with_llm.py
import sys
import os
import json
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.recovery_guide import OracleRecoveryGuide
from src.llm_engine import LLMEngine

class MockRAG:
    def retrieve_context(self, *args, **kwargs):
        return []

def test_llm_integration():
    print("üß™ TEST INT√âGRATION COMPL√àTE AVEC LLM")
    print("="*70)
    
    # 1. Initialisation du LLM
    print("üîß Initialisation du LLMEngine...")
    start_time = time.time()
    
    try:
        llm = LLMEngine(rag_setup=MockRAG(), default_model="tinyllama")
        init_time = time.time() - start_time
        print(f"‚úÖ LLM initialis√© en {init_time:.1f}s (mod√®le: tinyllama)")
    except Exception as e:
        print(f"‚ùå Erreur d'initialisation LLM: {e}")
        print("üí° Essayons avec gemma2:2b...")
        try:
            llm = LLMEngine(rag_setup=MockRAG(), default_model="gemma2:2b")
            print(f"‚úÖ LLM initialis√© avec gemma2:2b")
        except Exception as e2:
            print(f"‚ùå √âchec complet: {e2}")
            print("‚ö†Ô∏è  Utilisation du mode sans LLM")
            llm = None
    
    # 2. Cr√©ation du guide
    print("\nüîß Cr√©ation du OracleRecoveryGuide...")
    guide = OracleRecoveryGuide(llm_engine=llm, rag_setup=MockRAG())
    
    # 3. Test avec diff√©rentes questions
    test_cases = [
        {
            "question": "Comment r√©cup√©rer ma base au 15 mars 14h ?",
            "scenario": "pitr",
            "clarifications": {
                "target_time": "15-MAR-2024 14:00:00",
                "situation": "Suppression accidentelle de donn√©es critiques"
            }
        },
        {
            "question": "Ma base Oracle a crash√© apr√®s une panne √©lectrique",
            "scenario": "full_recovery",
            "clarifications": {
                "backups": "Backups RMAN complets sur disque NAS",
                "situation": "Crash complet suite √† coupure de courant"
            }
        },
        {
            "question": "J'ai supprim√© la table CLIENTS par erreur ce matin",
            "scenario": "table_recovery",
            "clarifications": {
                "table_name": "CLIENTS",
                "situation": "DROP TABLE ex√©cut√© accidentellement"
            }
        }
    ]
    
    for i, test in enumerate(test_cases, 1):
        print(f"\n{'='*70}")
        print(f"üß™ TEST {i}: {test['scenario'].upper()}")
        print(f"{'='*70}")
        print(f"üìù Question: {test['question']}")
        
        # √âtape 1: Sans clarifications
        print("\nüîÑ √âtape 1: Traitement initial...")
        step1_start = time.time()
        result1 = guide.handle_user_question(test['question'])
        step1_time = time.time() - step1_start
        
        print(f"   ‚è±Ô∏è  Temps: {step1_time:.1f}s")
        print(f"   üéØ Sc√©nario d√©tect√©: {result1.get('scenario_name', 'N/A')}")
        print(f"   ‚ùì Clarification n√©cessaire: {result1.get('needs_clarification', 'N/A')}")
        
        # Afficher les questions de clarification
        if result1.get('needs_clarification'):
            questions = result1.get('clarification_questions', [])
            print(f"   üìã Questions pos√©es ({len(questions)}):")
            for j, q in enumerate(questions[:3], 1):
                print(f"     {j}. {q}")
        
        # √âtape 2: Avec clarifications
        print(f"\nüîÑ √âtape 2: G√©n√©ration avec clarifications...")
        step2_start = time.time()
        result2 = guide.handle_user_question(test['question'], test['clarifications'])
        step2_time = time.time() - step2_start
        
        print(f"   ‚è±Ô∏è  Temps: {step2_time:.1f}s")
        
        # Analyser le r√©sultat
        guide_data = result2.get('guide', {})
        playbook = guide_data.get('playbook', {})
        
        print(f"\nüìä R√âSULTAT FINAL:")
        print(f"   üîß Mod√®le utilis√©: {guide_data.get('model_used', 'N/A')}")
        print(f"   üó£Ô∏è  Langue: {guide_data.get('language', 'N/A')}")
        print(f"   üìè Structur√©: {playbook.get('structured', False)}")
        print(f"   ‚è±Ô∏è  Temps estim√©: {playbook.get('estimated_time', 'N/A')}")
        
        # Afficher le contenu du playbook
        if playbook:
            print(f"\nüìã CONTENU DU PLAYBOOK:")
            
            # √âtapes
            steps = playbook.get('steps', [])
            if steps:
                print(f"   üìù √âTAPES ({len(steps)}):")
                for step in steps[:3]:  # Afficher 3 premi√®res √©tapes
                    if isinstance(step, dict):
                        print(f"     {step.get('number', '?')}. {step.get('description', '')}")
                    else:
                        print(f"     ‚Ä¢ {step}")
                if len(steps) > 3:
                    print(f"     ... et {len(steps) - 3} √©tapes suppl√©mentaires")
            
            # Commandes
            commands = playbook.get('commands', [])
            if commands:
                print(f"\n   üíª COMMANDES ({len(commands)}):")
                for cmd in commands[:3]:
                    print(f"     ‚Ä¢ {cmd}")
                if len(commands) > 3:
                    print(f"     ... et {len(commands) - 3} commandes suppl√©mentaires")
            
            # Points de validation
            validations = playbook.get('validation_points', [])
            if validations:
                print(f"\n   ‚úÖ POINTS DE VALIDATION ({len(validations)}):")
                for point in validations[:2]:
                    print(f"     ‚Ä¢ {point}")
            
            # V√©rifier les exigences
            print(f"\n{'='*70}")
            print("‚úÖ V√âRIFICATION DES EXIGENCES DU PROJET:")
            print(f"{'='*70}")
            
            requirements = {
                'playbook_structur√©': playbook.get('structured', False),
                '√©tapes_num√©rot√©es': len(steps) >= 3,
                'commandes_pr√©cises': len(commands) >= 2,
                'points_de_validation': len(validations) >= 1,
                'temps_estim√©': playbook.get('estimated_time') is not None,
                'r√©ponse_en_fran√ßais': guide_data.get('language') == 'french' or 'fran√ßais' in str(playbook.get('raw_response', '')).lower()
            }
            
            all_passed = True
            for req_name, req_met in requirements.items():
                status = "‚úì" if req_met else "‚úó"
                color = "\033[92m" if req_met else "\033[91m"  # Vert/Rouge
                reset = "\033[0m"
                print(f"   {color}{status}{reset} {req_name.replace('_', ' ').title()}")
                if not req_met:
                    all_passed = False
            
            if all_passed:
                print(f"\nüéâ Toutes les exigences sont satisfaites !")
            else:
                print(f"\n‚ö†Ô∏è  Certaines exigences ne sont pas satisfaites")
                
                # Afficher un extrait de la r√©ponse brute pour debug
                raw_response = playbook.get('raw_response', '')
                if raw_response:
                    print(f"\nüìÑ Extrait de la r√©ponse LLM (200 premiers caract√®res):")
                    print(f"   '{raw_response[:200]}...'")
        
        else:
            print(f"‚ùå Aucun playbook g√©n√©r√©")
            print(f"   Guide disponible: {list(guide_data.keys())}")
        
        # Pause entre les tests
        if i < len(test_cases):
            print(f"\n‚è≥ Pause de 2 secondes avant le test suivant...")
            time.sleep(2)

def test_specific_demo_question():
    """Test sp√©cifique pour la question de d√©mo requise"""
    print(f"\n{'='*70}")
    print("üß™ TEST SP√âCIAL: Question de validation du projet")
    print(f"{'='*70}")
    
    print("üìã Exigence du projet: 'Validation : peut r√©pondre \"Comment r√©cup√©rer ma base au 15 mars 14h ?\"'")
    
    # Initialiser avec LLM
    try:
        llm = LLMEngine(rag_setup=MockRAG(), default_model="tinyllama")
        guide = OracleRecoveryGuide(llm_engine=llm, rag_setup=MockRAG())
    except:
        print("‚ö†Ô∏è  LLM non disponible, test sans LLM")
        guide = OracleRecoveryGuide(llm_engine=None)
    
    # La question exacte de l'exigence
    demo_question = "Comment r√©cup√©rer ma base au 15 mars 14h ?"
    clarifications = {
        "target_time": "15-MAR-2024 14:00:00",
        "situation": "D√©monstration de la plateforme de r√©cup√©ration Oracle IA"
    }
    
    print(f"\n‚ùì Question: {demo_question}")
    print(f"üìã Clarifications fournies:")
    for k, v in clarifications.items():
        print(f"   ‚Ä¢ {k}: {v}")
    
    print("\nüîÑ G√©n√©ration du playbook...")
    start_time = time.time()
    result = guide.handle_user_question(demo_question, clarifications)
    elapsed = time.time() - start_time
    
    print(f"‚è±Ô∏è  Temps de g√©n√©ration: {elapsed:.1f}s")
    
    # Analyse d√©taill√©e
    guide_data = result.get('guide', {})
    playbook = guide_data.get('playbook', {})
    
    print(f"\nüìä ANALYSE DE LA R√âPONSE:")
    print(f"   Sc√©nario: {result.get('scenario_name')}")
    print(f"   Mod√®le: {guide_data.get('model_used', 'local_fallback')}")
    
    # V√©rifier sp√©cifiquement les commandes RMAN
    commands = playbook.get('commands', [])
    rman_commands = [cmd for cmd in commands if 'rman>' in cmd.lower()]
    
    print(f"\nüíª COMMANDES RMAN TROUV√âES ({len(rman_commands)}):")
    for cmd in rman_commands[:5]:
        print(f"   ‚Ä¢ {cmd}")
    
    # V√©rifier la pr√©sence de SET UNTIL TIME (sp√©cifique √† PITR)
    has_set_until = any('set until' in cmd.lower() for cmd in commands)
    print(f"\nüîç V√âRIFICATIONS SP√âCIFIQUES PITR:")
    print(f"   ‚úì SET UNTIL TIME pr√©sent: {has_set_until}")
    print(f"   ‚úì Date sp√©cifique (15 mars): {'15' in str(commands)}")
    print(f"   ‚úì Heure sp√©cifique (14h): {'14' in str(commands)}")
    
    # Afficher un exemple de playbook g√©n√©r√©
    print(f"\nüìã EXEMPLE DE PLAYBOOK G√âN√âR√â:")
    if playbook.get('steps'):
        print(f"\n√âtapes de r√©cup√©ration:")
        for step in playbook['steps'][:5]:
            if isinstance(step, dict):
                print(f"  {step.get('number')}. {step.get('description')}")
    
    print(f"\n‚è±Ô∏è  Temps estim√© de r√©cup√©ration: {playbook.get('estimated_time', 'N/A')}")
    
    # Conclusion
    print(f"\n{'='*70}")
    print("üéØ CONCLUSION DU TEST DE VALIDATION:")
    print(f"{'='*70}")
    
    if len(rman_commands) >= 2 and has_set_until:
        print("‚úÖ SUCC√àS: Le Module 8 r√©pond correctement √† la question de validation!")
        print("   ‚úì G√©n√®re un playbook structur√©")
        print("   ‚úì Inclut des commandes RMAN sp√©cifiques")
        print("   ‚úì G√®re le sc√©nario PITR avec date/heure pr√©cise")
        print("   ‚úì Pr√™t pour l'int√©gration dans le dashboard")
    else:
        print("‚ö†Ô∏è  ATTENTION: R√©ponse incompl√®te")
        print("   ‚Ä¢ V√©rifier les prompts dans prompts.yaml")
        print("   ‚Ä¢ S'assurer que le LLM r√©pond en fran√ßais")
        print("   ‚Ä¢ Ajouter plus d'exemples dans les prompts")

def quick_performance_test():
    """Test rapide de performance"""
    print(f"\n{'='*70}")
    print("‚ö° TEST DE PERFORMANCE RAPIDE")
    print(f"{'='*70}")
    
    questions = [
        "Crash base Oracle",
        "R√©cup√©ration table",
        "PITR 14h",
    ]
    
    try:
        llm = LLMEngine(rag_setup=MockRAG(), default_model="tinyllama")
        guide = OracleRecoveryGuide(llm_engine=llm, rag_setup=MockRAG())
        model_name = "tinyllama"
    except:
        guide = OracleRecoveryGuide(llm_engine=None)
        model_name = "sans_llm"
    
    print(f"Mod√®le: {model_name}")
    print(f"{'Question':<30} {'Sc√©nario':<20} {'Temps (s)':<10} {'√âtapes':<10}")
    print("-" * 70)
    
    for question in questions:
        start = time.time()
        result = guide.handle_user_question(question)
        elapsed = time.time() - start
        
        scenario = result.get('scenario', 'unknown')
        scenario_name = guide.scenarios.get(scenario, 'unknown')
        steps = len(result.get('guide', {}).get('playbook', {}).get('steps', []))
        
        print(f"{question[:28]:<30} {scenario_name[:18]:<20} {elapsed:<10.1f} {steps:<10}")

if __name__ == "__main__":
    print("üöÄ TEST COMPLET DU MODULE 8 AVEC LLM")
    print("Version: Int√©gration LLM + Playbook structur√©\n")
    
    try:
        # Test principal
        test_llm_integration()
        
        # Test sp√©cifique de validation
        test_specific_demo_question()
        
        # Test de performance
        quick_performance_test()
        
        print(f"\n{'='*70}")
        print("üìã R√âSUM√â POUR L'INT√âGRATION DANS LE DASHBOARD:")
        print(f"{'='*70}")
        print("""
        ‚úÖ FONCTIONNALIT√âS TEST√âES:
          1. Classification des 4 sc√©narios
          2. Questions de clarification intelligentes
          3. G√©n√©ration de playbooks structur√©s
          4. Commandes RMAN exactes
          5. Temps estim√© de r√©cup√©ration
          6. Points de validation
        
        üîß INT√âGRATION DANS DASHBOARD:
        
        from src.llm_engine import LLMEngine
        from src.recovery_guide import OracleRecoveryGuide
        
        # Initialisation
        llm = LLMEngine(rag_setup=None)  # ou avec RAG si disponible
        recovery_module = OracleRecoveryGuide(llm_engine=llm)
        
        # Utilisation
        def handle_recovery_request(user_question, clarifications=None):
            result = recovery_module.handle_user_question(user_question, clarifications)
            
            if result['needs_clarification']:
                # Demander les clarifications √† l'utilisateur
                return {
                    'type': 'clarification',
                    'questions': result['clarification_questions'],
                    'scenario': result['scenario_name']
                }
            else:
                # Afficher le playbook
                playbook = result['guide']['playbook']
                return {
                    'type': 'playbook',
                    'scenario': result['scenario_name'],
                    'steps': playbook['steps'],
                    'commands': playbook['commands'],
                    'validation_points': playbook['validation_points'],
                    'estimated_time': playbook['estimated_time']
                }
        """)
        
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Test interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur lors du test: {e}")
        import traceback
        traceback.print_exc()