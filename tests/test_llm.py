#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TEST MODULE 2 : V√©rification RAG Setup avec recherche s√©mantique
Ce script teste la recherche vectorielle dans votre index Pinecone
"""

import sys
import time
from pathlib import Path

# Ajouter le chemin parent pour importer vos modules
sys.path.append(str(Path(__file__).parent.parent))

# Importer vos modules
from rag_setup import OracleRAGSetup
from llm_engine import LLMEngine

def test_basic_semantic_search():
    """Test basique de recherche s√©mantique"""
    print("\n" + "="*80)
    print("üß™ TEST 1: Recherche s√©mantique basique")
    print("="*80)
    
    try:
        # Initialiser RAG setup
        rag = OracleRAGSetup(namespace="module2")
        
        # V√©rifier les stats de l'index
        print("\nüìä Statistiques de l'index:")
        rag.get_stats()
        
        # Requ√™tes de test
        test_queries = [
            "optimisation requ√™te SQL",
            "index Oracle performance",
            "s√©curit√© base de donn√©es",
            "backup RMAN strat√©gie",
            "analyse plan d'ex√©cution"
        ]
        
        print("\nüîç Test de recherche s√©mantique:")
        for query in test_queries:
            print(f"\n{'‚îÄ'*40}")
            print(f"‚ùì Requ√™te: '{query}'")
            results = rag.retrieve_context(query, n_results=3, min_score=0.1)
            
            if results:
                print(f"‚úÖ {len(results)} r√©sultat(s) trouv√©(s):")
                for i, result in enumerate(results, 1):
                    print(f"\n  üìå R√©sultat {i}:")
                    print(f"     Score: {result['score']:.4f}")
                    print(f"     Titre: {result['metadata']['title']}")
                    print(f"     Source: {result['metadata']['source']}")
                    print(f"     Extrait: {result['content'][:100]}...")
            else:
                print(f"‚ùå Aucun r√©sultat pour cette requ√™te")
    
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False
    
    return True

def test_llm_integration():
    """Test d'int√©gration avec LLM"""
    print("\n" + "="*80)
    print("üß™ TEST 2: Int√©gration LLM avec RAG")
    print("="*80)
    
    try:
        # Initialiser LLM Engine
        llm = LLMEngine()
        
        # Initialiser RAG
        rag = OracleRAGSetup(namespace="module2")
        
        
        
        # Questions de test avec RAG
        test_questions = [
            "Comment optimiser une requ√™te lente avec des index?",
            "Quelles sont les meilleures pratiques de s√©curit√© Oracle?",
            "Comment fonctionne RMAN pour les backups?"
        ]
        
        for question in test_questions:
            print(f"\n{'‚îÄ'*40}")
            print(f"ü§ñ Question: {question}")
            
            # Recherche vectorielle
            print("üîç Recherche dans Pinecone...")
            context_results = rag.retrieve_context(question, n_results=3)
            
            if context_results:
                print(f"‚úÖ {len(context_results)} contexte(s) trouv√©(s)")
                
                # Afficher les r√©sultats de recherche
                print("\nüìä R√©sultats de recherche vectorielle:")
                for i, result in enumerate(context_results, 1):
                    print(f"  {i}. [{result['score']:.4f}] {result['metadata']['title']}")
                
                # G√©n√©rer r√©ponse avec contexte
                print("\nüí≠ G√©n√©ration de r√©ponse avec contexte...")
                response = llm.query_with_vector_context(
                    user_prompt=question,
                    vector_results=context_results,
                    show_results=False  # On affichera manuellement
                )
                
                # Afficher la r√©ponse
                print("\nüí° R√©ponse LLM:")
                print(f"  {response[:200]}...")
            else:
                print("‚ùå Aucun contexte trouv√© pour cette question")
    
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False
    
    return True

def test_similarity_scores():
    """Test des scores de similarit√©"""
    print("\n" + "="*80)
    print("üß™ TEST 3: Analyse des scores de similarit√©")
    print("="*80)
    
    try:
        rag = OracleRAGSetup(namespace="module2")
        
        # Paire de requ√™tes avec similarit√© attendue
        query_pairs = [
            ("optimisation requ√™te", "performance SQL"),
            ("s√©curit√© Oracle", "audit base de donn√©es"),
            ("backup RMAN", "strat√©gie sauvegarde")
        ]
        
        for query1, query2 in query_pairs:
            print(f"\n{'‚îÄ'*40}")
            print(f"Comparaison: '{query1}' vs '{query2}'")
            
            # R√©sultats pour query1
            results1 = rag.retrieve_context(query1, n_results=1, min_score=0)
            results2 = rag.retrieve_context(query2, n_results=1, min_score=0)
            
            if results1 and results2:
                doc1 = results1[0]
                doc2 = results2[0]
                
                print(f"üìå Document trouv√© pour '{query1}':")
                print(f"   Titre: {doc1['metadata']['title']}")
                print(f"   Score: {doc1['score']:.4f}")
                
                print(f"\nüìå Document trouv√© pour '{query2}':")
                print(f"   Titre: {doc2['metadata']['title']}")
                print(f"   Score: {doc2['score']:.4f}")
                
                # V√©rifier si c'est le m√™me document
                if doc1['id'] == doc2['id']:
                    print(f"\n‚úÖ M√™me document retrouv√© pour les deux requ√™tes")
                else:
                    print(f"\n‚ö†Ô∏è  Documents diff√©rents trouv√©s")
            else:
                print("‚ùå Pas assez de r√©sultats pour comparer")
    
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False
    
    return True

def test_edge_cases():
    """Test des cas limites"""
    print("\n" + "="*80)
    print("üß™ TEST 4: Cas limites et erreurs")
    print("="*80)
    
    try:
        rag = OracleRAGSetup(namespace="module2")
        
        # Test avec requ√™te vide
        print("\n‚ùì Test avec requ√™te vide:")
        results = rag.retrieve_context("", n_results=3)
        print(f"R√©sultats: {len(results)}")
        
        # Test avec requ√™te tr√®s longue
        print("\n‚ùì Test avec requ√™te tr√®s longue:")
        long_query = " " + "optimisation " * 50 + " "
        results = rag.retrieve_context(long_query, n_results=3)
        print(f"R√©sultats: {len(results)}")
        
        # Test avec caract√®res sp√©ciaux
        print("\n‚ùì Test avec caract√®res sp√©ciaux:")
        special_query = "SQL injection ' OR '1'='1"
        results = rag.retrieve_context(special_query, n_results=3)
        print(f"R√©sultats: {len(results)}")
        
        # Test avec min_score √©lev√©
        print("\n‚ùì Test avec min_score=0.8 (tr√®s restrictif):")
        results = rag.retrieve_context("Oracle", n_results=3, min_score=0.8)
        print(f"R√©sultats avec score >0.8: {len(results)}")
        
        # Test avec min_score bas
        print("\n‚ùì Test avec min_score=0.0 (peu restrictif):")
        results = rag.retrieve_context("Oracle", n_results=3, min_score=0.0)
        print(f"R√©sultats avec score >0.0: {len(results)}")
        
        if results:
            print("\nüìä Distribution des scores:")
            for result in results:
                print(f"  Score: {result['score']:.4f}")
    
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False
    
    return True

def test_complete_workflow():
    """Test du workflow complet"""
    print("\n" + "="*80)
    print("üß™ TEST 5: Workflow complet RAG + LLM")
    print("="*80)
    
    try:
        # Initialiser les deux modules
        llm = LLMEngine()
        rag = OracleRAGSetup(namespace="module2")
        
        # Workflow complet
        user_question = "Comment cr√©er un index pour am√©liorer les performances?"
        
        print(f"\nüë§ Question utilisateur: {user_question}")
        
        # √âtape 1: Recherche vectorielle
        print("\n1Ô∏è‚É£ Recherche vectorielle dans Pinecone...")
        context_results = rag.retrieve_context(user_question, n_results=3, min_score=0.3)
        
        if not context_results:
            print("‚ùå Aucun contexte trouv√©. Utilisation de LLM seul.")
            response = llm.generate(user_question)
            print(f"\nü§ñ R√©ponse (sans contexte):\n{response}")
            return True
        
        print(f"‚úÖ {len(context_results)} contexte(s) trouv√©(s)")
        
        # √âtape 2: Affichage des r√©sultats vectoriels
        print("\n2Ô∏è‚É£ R√©sultats de recherche vectorielle:")
        print("-"*60)
        
        for i, result in enumerate(context_results, 1):
            print(f"\nüìå R√©sultat {i}:")
            print(f"   Score: {result['score']:.4f}")
            print(f"   Titre: {result['metadata']['title']}")
            print(f"   Source: {result['metadata']['source']}")
            print(f"   Extrait: {result['content'][:150]}...")
        
        print("-"*60)
        
        # √âtape 3: G√©n√©ration avec LLM
        print("\n3Ô∏è‚É£ G√©n√©ration de r√©ponse avec LLM...")
        
        # Pr√©parer le contexte format√©
        formatted_context = ""
        for i, result in enumerate(context_results, 1):
            formatted_context += f"[Document {i}]\n"
            formatted_context += f"Titre: {result['metadata']['title']}\n"
            formatted_context += f"Contenu: {result['content'][:500]}\n"
            formatted_context += f"Score de pertinence: {result['score']:.4f}\n"
            formatted_context += "-"*40 + "\n"
        
        # G√©n√©rer la r√©ponse
        prompt = f"""En utilisant les documents contextuels ci-dessous, r√©ponds √† la question de l'utilisateur.

Documents contextuels:
{formatted_context}

Question: {user_question}

R√©ponse:"""
        
        response = llm.generate(prompt)
        
        print("\n4Ô∏è‚É£ R√©ponse finale:")
        print("="*60)
        print(response)
        print("="*60)
        
        # √âtape 4: √âvaluation de la qualit√©
        print("\n5Ô∏è‚É£ √âvaluation de la r√©ponse:")
        evaluation_prompt = f"""√âvalue la qualit√© de cette r√©ponse bas√©e sur les crit√®res suivants:
1. Pertinence par rapport √† la question
2. Utilisation du contexte fourni
3. Pr√©cision technique
4. Clart√© de l'explication

Question: {user_question}

R√©ponse: {response[:500]}...

Note sur 10 et commentaires:"""
        
        evaluation = llm.generate(evaluation_prompt)
        print(f"\nüìà √âvaluation:\n{evaluation}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur dans le workflow: {e}")
        return False

def run_all_tests():
    """Ex√©cuter tous les tests"""
    print("\n" + "="*80)
    print("üöÄ LANCEMENT DE TOUS LES TESTS RAG")
    print("="*80)
    
    tests = [
        ("Recherche s√©mantique basique", test_basic_semantic_search),
        ("Int√©gration LLM", test_llm_integration),
        ("Scores de similarit√©", test_similarity_scores),
        ("Cas limites", test_edge_cases),
        ("Workflow complet", test_complete_workflow)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n‚ñ∂Ô∏è  D√©but test: {test_name}")
        start_time = time.time()
        
        try:
            success = test_func()
            elapsed = time.time() - start_time
            
            if success:
                print(f"‚úÖ {test_name}: R√âUSSI ({elapsed:.2f}s)")
                results.append((test_name, True, elapsed))
            else:
                print(f"‚ùå {test_name}: √âCHEC ({elapsed:.2f}s)")
                results.append((test_name, False, elapsed))
                
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"üí• {test_name}: ERREUR - {e} ({elapsed:.2f}s)")
            results.append((test_name, False, elapsed))
    
    # R√©sum√©
    print("\n" + "="*80)
    print("üìä R√âSUM√â DES TESTS")
    print("="*80)
    
    passed = sum(1 for _, success, _ in results if success)
    total = len(results)
    
    print(f"\nüìà R√©sultats: {passed}/{total} tests r√©ussis ({passed/total*100:.1f}%)")
    
    for test_name, success, elapsed in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"  {status} {test_name:<30} {elapsed:.2f}s")
    
    print("\n" + "="*80)
    if passed == total:
        print("üéâ TOUS LES TESTS SONT R√âUSSIS ! Votre RAG setup fonctionne correctement.")
    else:
        print("‚ö†Ô∏è  Certains tests ont √©chou√©. V√©rifiez votre configuration.")
    print("="*80)

if __name__ == "__main__":
    # Ex√©cuter soit tous les tests, soit un test sp√©cifique
    if len(sys.argv) > 1:
        test_name = sys.argv[1]
        if test_name == "basic":
            test_basic_semantic_search()
        elif test_name == "llm":
            test_llm_integration()
        elif test_name == "scores":
            test_similarity_scores()
        elif test_name == "edge":
            test_edge_cases()
        elif test_name == "workflow":
            test_complete_workflow()
        else:
            print(f"Test '{test_name}' non reconnu. Utilisez: basic, llm, scores, edge, workflow")
    else:
        # Ex√©cuter tous les tests par d√©faut
        run_all_tests()